<!DOCTYPE html>
<html>
  <head>
    <title>My Basic Website</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands"></script>
  </head>
  <body>
    <header>
      <h1>Welcome to My Website</h1>
    </header>

    <nav>
      <ul>
        <li><a href="page1.html">Page 1</a></li>
        <li><a href="page2.html">Page 2</a></li>
        <li><a href="page3.html">Page 3</a></li>
        <li><a href="page4.html">Page 4</a></li>
        <li><a href="page5.html">Page 5</a></li>
        <li><a href="page6.html">Page 6</a></li>
        <li><a href="page7.html">Page 7</a></li>
        <li><a href="page8.html">Page 8</a></li>
        <li><a href="page9.html">Page 9</a></li>
        <li><a href="page10.html">Page 10</a></li>
      </ul>
    </nav>

    <main>
      <h1>Audio Recorder</h1>
      <audio controls></audio>
      <button id="toggleRecording" onclick="toggleAudioRecording()">
        Start Recording
      </button>

      <button id="playRecording" style="margin-top: 10px" disabled>
        Play Recording
      </button>

      <div id="classificationResult" style="margin-top: 20px">
        <h2>Classification Result:</h2>
        <p id="resultText">No result yet</p>
      </div>

      <script>
        let recognizer;
        const playRecordingButton = document.getElementById("playRecording");

        // Function to load the model
        async function loadModel() {
          if (!recognizer) {
            recognizer = await speechCommands.create("BROWSER_FFT");
            await recognizer.ensureModelLoaded();
            console.log("Speech command model loaded.");
          }
        }

        window.onload = loadModel;

        const audioChunks = [];
        let mediaRecorder;
        let audioContext;
        let recording = false;

        const toggleRecordingButton =
          document.getElementById("toggleRecording");
        const audioElement = document.querySelector("audio");

        function toggleAudioRecording() {
          if (recording) {
            // Stop recording
            mediaRecorder.stop();
            recording = false;
            toggleRecordingButton.textContent = "Start Recording";
          } else {
            // Start recording
            audioChunks.length = 0;
            audioContext = new (window.AudioContext ||
              window.webkitAudioContext)();
            navigator.mediaDevices
              .getUserMedia({ audio: true })
              .then(function (stream) {
                mediaRecorder = new MediaRecorder(stream);

                mediaRecorder.ondataavailable = (event) => {
                  if (event.data.size > 0) {
                    audioChunks.push(event.data);
                  }
                };

                mediaRecorder.onstop = async () => {
                  console.log("Recorder stopped, processing the audio");
                  const audioBlob = new Blob(audioChunks, {
                    type: "audio/wav",
                  });

                  const audioUrl = URL.createObjectURL(audioBlob);

                  audioElement.src = audioUrl;
                  playRecordingButton.disabled = false;
                  playRecordingButton.onclick = () => {
                    audioElement.play();
                  };
                  console.log("Play button enabled");

                  if (mediaRecorder.stream) {
                    mediaRecorder.stream.getTracks().forEach((track) => {
                      console.log("Stopping track: ", track);
                      track.stop();
                    });
                  }

                  if (!recognizer) {
                    console.log("Recognizer not loaded.");
                    return;
                  }

                  // Convert the audioBlob to a format that the model can process
                  const mySpectrogramData = await convertAudioToSpectrogram(
                    audioBlob,
                    recognizer
                  );
                  console.log("Audio converted successfully.");

                  console.log(mySpectrogramData);
                  const tensor = tf.tensor4d(
                    mySpectrogramData,
                    [1, 43, 232, 1]
                  );

                  console.log("tensor created successfully.");

                  const output = await recognizer.recognize(tensor);

                  console.log(output.scores);

                  // Find the label with the highest probability
                  const scores = output.scores;
                  const labels = recognizer.wordLabels();
                  const highestLabelIndex = scores.indexOf(Math.max(...scores));
                  const highestLabel = labels[highestLabelIndex];

                  // Display the result
                  const resultTextElement =
                    document.getElementById("resultText");
                  resultTextElement.textContent = `Most probable class: ${highestLabel}`;

                  tf.dispose([tensor, output]); // Dispose the tensors
                };

                mediaRecorder.start();
                recording = true;
                toggleRecordingButton.textContent = "Stop Recording";
              })
              .catch(function (error) {
                console.error("Error accessing the microphone: " + error);
              });
          }
        }

        async function convertAudioToSpectrogram(audioBlob, recognizer) {
          console.log("convertAudioToSpectrogram");
          const audioBuffer = await decodeAudioData(audioBlob);
          console.log("audioBuffer");
          let spectrogram = await generateSpectrogram(
            audioBuffer,
            recognizer.params().fftSize
          );
          console.log("spectrogram");
          // Ensure spectrogram has 43 time frames
          spectrogram = spectrogram.slice(0, 43);
          while (spectrogram.length < 43) {
            spectrogram.push(new Array(232).fill(-100)); // Padding with low values
          }

          // Ensure each frequency array has 232 frequency bins
          spectrogram = spectrogram.map((freqArray) => {
            const lengthDiff = 232 - freqArray.length;
            return lengthDiff > 0
              ? freqArray.concat(new Array(lengthDiff).fill(-100))
              : freqArray.slice(0, 232);
          });

          // Format the spectrogram to be a 4D array: [1, 43, 232, 1]
          const reshapedSpectrogram = [
            spectrogram.map((timeFrame) =>
              timeFrame.map((freqVal) => [freqVal])
            ),
          ];
          return reshapedSpectrogram;
        }

        async function decodeAudioData(audioBlob) {
          const audioContext = new (window.AudioContext ||
            window.webkitAudioContext)();
          const arrayBuffer = await audioBlob.arrayBuffer();
          return audioContext.decodeAudioData(arrayBuffer);
        }

        function generateSpectrogram(audioBuffer, fftSize = 2048) {
          console.log("generateSpectrogram - Start");
          return new Promise((resolve, reject) => {
            const audioContext = new (window.AudioContext ||
              window.webkitAudioContext)();
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = fftSize;

            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Float32Array(bufferLength);

            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(analyser);
            analyser.connect(audioContext.destination); // Connect analyser to the destination

            const spectrogram = [];

            let frameCount = 0;
            const maxFrames = 60; // Adjust this based on expected audio duration

            const gatherData = () => {
              console.log("Gathering data...");
              analyser.getFloatFrequencyData(dataArray);
              spectrogram.push([...dataArray]);
              frameCount++;

              if (frameCount < maxFrames) {
                requestAnimationFrame(gatherData);
              } else {
                console.log("Stopping data collection");
                source.stop();
                audioContext.close();
                resolve(spectrogram);
              }
            };

            console.log("Starting audio source");
            source.start();
            gatherData();
          });
        }
      </script>
    </main>
  </body>
</html>
